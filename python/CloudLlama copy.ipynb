{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c1ea03a-cc69-45b0-80d3-664e48ca6831",
   "metadata": {},
   "source": [
    "## This notebook contains:\n",
    "* Running Llama2 in the cloud hosted on Replicate\n",
    "* Using LangChain to ask Llama general questions and follow up questions\n",
    "* Using LangChain to load PDF docs - (Kenya Law Documents) - and chat about it.\n",
    "* The end result will be a chatbot that will be able answer questions about the data not publicly available when Llama2 was trained, or about your own data. RAG is one way to prevent LLM's hallucination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61dde626",
   "metadata": {},
   "source": [
    "Let's start by installing the necessary packages:\n",
    "- sentence-transformers for text embeddings\n",
    "- chromadb gives us database capabilities \n",
    "- langchain provides necessary RAG tools for this demo\n",
    "\n",
    "And setting up the Replicate token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c608df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-core in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (0.2.2)\n",
      "Requirement already satisfied: replicate in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (0.25.0)\n",
      "Requirement already satisfied: sentence-transformers in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (2.5.1)\n",
      "Requirement already satisfied: chromadb in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (0.4.24)\n",
      "Requirement already satisfied: pypdf in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (4.1.0)\n",
      "Requirement already satisfied: faiss-cpu in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (1.8.0)\n",
      "Requirement already satisfied: langchain-elasticsearch in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (0.2.2)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from langchain-core) (6.0.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from langchain-core) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from langchain-core) (0.1.64)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from langchain-core) (23.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from langchain-core) (2.6.4)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from langchain-core) (8.2.3)\n",
      "Requirement already satisfied: httpx<1,>=0.21.0 in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from replicate) (0.27.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from replicate) (4.10.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.32.0 in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from sentence-transformers) (4.38.2)\n",
      "Requirement already satisfied: tqdm in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from sentence-transformers) (4.66.2)\n",
      "Requirement already satisfied: torch>=1.11.0 in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from sentence-transformers) (2.2.1)\n",
      "Requirement already satisfied: numpy in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from sentence-transformers) (1.4.1.post1)\n",
      "Requirement already satisfied: scipy in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from sentence-transformers) (1.12.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.15.1 in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from sentence-transformers) (0.21.4)\n",
      "Requirement already satisfied: Pillow in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from sentence-transformers) (10.2.0)\n",
      "Requirement already satisfied: build>=1.0.3 in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from chromadb) (1.1.1)\n",
      "Requirement already satisfied: requests>=2.28 in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from chromadb) (2.31.0)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.3 in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from chromadb) (0.7.3)\n",
      "Requirement already satisfied: fastapi>=0.95.2 in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from chromadb) (0.110.0)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.29.0)\n",
      "Requirement already satisfied: posthog>=2.4.0 in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from chromadb) (3.5.0)\n",
      "Requirement already satisfied: pulsar-client>=3.1.0 in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from chromadb) (3.4.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from chromadb) (1.17.1)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from chromadb) (1.23.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from chromadb) (1.23.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from chromadb) (0.44b0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from chromadb) (1.23.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from chromadb) (0.15.2)\n",
      "Requirement already satisfied: pypika>=0.48.9 in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from chromadb) (6.3.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from chromadb) (1.62.1)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from chromadb) (4.1.2)\n",
      "Requirement already satisfied: typer>=0.9.0 in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from chromadb) (0.9.0)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from chromadb) (29.0.0)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from chromadb) (4.1.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from chromadb) (3.9.15)\n",
      "Requirement already satisfied: elasticsearch<9.0.0,>=8.13.1 in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from elasticsearch[vectorstore-mmr]<9.0.0,>=8.13.1->langchain-elasticsearch) (8.13.2)\n",
      "Requirement already satisfied: pyproject_hooks in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from build>=1.0.3->chromadb) (1.0.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.6 in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from build>=1.0.3->chromadb) (6.11.0)\n",
      "Requirement already satisfied: tomli>=1.1.0 in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from build>=1.0.3->chromadb) (2.0.1)\n",
      "Requirement already satisfied: elastic-transport<9,>=8.13 in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from elasticsearch<9.0.0,>=8.13.1->elasticsearch[vectorstore-mmr]<9.0.0,>=8.13.1->langchain-elasticsearch) (8.13.1)\n",
      "\u001b[33mWARNING: elasticsearch 8.13.2 does not provide the extra 'vectorstore-mmr'\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: starlette<0.37.0,>=0.36.3 in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from fastapi>=0.95.2->chromadb) (0.36.3)\n",
      "Requirement already satisfied: anyio in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from httpx<1,>=0.21.0->replicate) (4.3.0)\n",
      "Requirement already satisfied: certifi in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from httpx<1,>=0.21.0->replicate) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from httpx<1,>=0.21.0->replicate) (1.0.4)\n",
      "Requirement already satisfied: idna in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from httpx<1,>=0.21.0->replicate) (3.6)\n",
      "Requirement already satisfied: sniffio in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from httpx<1,>=0.21.0->replicate) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from httpcore==1.*->httpx<1,>=0.21.0->replicate) (0.14.0)\n",
      "Requirement already satisfied: filelock in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2024.3.1)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from jsonpatch<2.0,>=1.33->langchain-core) (2.4)\n",
      "Requirement already satisfied: six>=1.9.0 in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from kubernetes>=28.1.0->chromadb) (2.9.0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from kubernetes>=28.1.0->chromadb) (2.28.2)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from kubernetes>=28.1.0->chromadb) (1.7.0)\n",
      "Requirement already satisfied: requests-oauthlib in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from kubernetes>=28.1.0->chromadb) (1.4.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from kubernetes>=28.1.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: coloredlogs in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from onnxruntime>=1.14.1->chromadb) (24.3.7)\n",
      "Requirement already satisfied: protobuf in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from onnxruntime>=1.14.1->chromadb) (4.25.3)\n",
      "Requirement already satisfied: sympy in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.14)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.63.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.23.0 in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.23.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.23.0 in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.23.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.44b0 in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.44b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.44b0 in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.44b0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.44b0 in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.44b0)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.44b0 in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.44b0)\n",
      "Requirement already satisfied: setuptools>=16.0 in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from opentelemetry-instrumentation==0.44b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (69.2.0)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from opentelemetry-instrumentation==0.44b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: asgiref~=3.0 in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from opentelemetry-instrumentation-asgi==0.44b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.7.2)\n",
      "Requirement already satisfied: monotonic>=1.5 in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from pydantic<3,>=1->langchain-core) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from pydantic<3,>=1->langchain-core) (2.16.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from requests>=2.28->chromadb) (3.3.2)\n",
      "Requirement already satisfied: networkx in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (2023.12.25)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.4.2)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
      "Requirement already satisfied: httptools>=0.5.0 in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.1)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.1)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.19.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
      "Requirement already satisfied: websockets>=10.4 in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (12.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from importlib-resources->chromadb) (3.17.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from scikit-learn->sentence-transformers) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from scikit-learn->sentence-transformers) (3.3.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from anyio->httpx<1,>=0.21.0->replicate) (1.2.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.5.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain-core replicate sentence-transformers chromadb pypdf faiss-cpu langchain-elasticsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8870c1",
   "metadata": {},
   "source": [
    "Next we call the Llama 2 model from replicate. In this example we will use the llama 2 13b chat model. You can find more Llama 2 models by searching for them on the [Replicate model explore page](https://replicate.com/explore?query=llama).\n",
    "\n",
    "The model is added in the format: model_name/version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb042eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Replicate API token...\n",
      "Replicate API token set.\n",
      "r8_Mrjuqk93LhNWTypoogA8o0JNCiywa1822TSSn\n",
      "Initializing Llama3 model...\n",
      "Llama3 model initialized.\n"
     ]
    }
   ],
   "source": [
    "import replicate\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access the environment variable\n",
    "REPLICATE_API_TOKEN = os.getenv('REPLICATE_API_TOKEN')\n",
    "\n",
    "print(\"Getting Replicate API token...\")\n",
    "REPLICATE_API_TOKEN = \"r8_Mrjuqk93LhNWTypoogA8o0JNCiywa1822TSSn\"\n",
    "os.environ[\"REPLICATE_API_TOKEN\"] = REPLICATE_API_TOKEN\n",
    "print(\"Replicate API token set.\")\n",
    "print((REPLICATE_API_TOKEN))\n",
    "\n",
    "print(\"Initializing Llama3 model...\")\n",
    "llm = replicate.models.get(\"meta/meta-llama-3-70b-instruct\")\n",
    "print(\"Llama3 model initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39c7b615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Finance Act 2024 of Kenya is a legislation that outlines the country's tax policies and reforms for the fiscal year 2024, aiming to promote economic growth, reduce inequality, and increase government revenue.\n"
     ]
    }
   ],
   "source": [
    "# Prepare the input\n",
    "chat_history = []  # Initialize an empty list to store chat history\n",
    "input = {\n",
    "    \"prompt\": \"Very briefly in 1 sentence, explain about the finance act 2024 of Kenya\",\n",
    "    \"prompt_template\": \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    "    \"temperature\": 0.75,\n",
    "    \"top_p\": 0.9,\n",
    "    \"max_tokens\": 800,\n",
    "    \"chat_history\": chat_history  \n",
    "}\n",
    "\n",
    "# To run the model and get the output\n",
    "output = replicate.run(llm, input=input)\n",
    "print(\"\".join(output))\n",
    "\n",
    "# Update the chat history\n",
    "chat_history.append({\"user\": input[\"prompt\"], \"assistant\": \"\".join(output)})\n",
    "\n",
    "# To stream the output\n",
    "# for event in replicate.stream(llm, input=input):\n",
    "#     print(event, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd207c80",
   "metadata": {},
   "source": [
    "With the model set up, it is now possible to ask some questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "493a7148",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Model' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwho wrote the book Innovator\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms dilemma?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(answer)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Model' object is not callable"
     ]
    }
   ],
   "source": [
    "question = \"who wrote the book Innovator's dilemma?\"\n",
    "answer = llm(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f315f000",
   "metadata": {},
   "source": [
    "We will then try to follow up the response with a question asking for more information on the book. \n",
    "\n",
    "Since the chat history is not passed on Llama doesn't have the context and doesn't know this is more about the book thus it treats this as new query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b5c8676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hello! As a helpful assistant, I'm here to assist you with any questions or tasks you may have. Whether it's providing information on a wide range of topics, helping you with a project or task, or simply being a listening ear, I'm here to help in any way I can.\n",
      "\n",
      "I have access to a vast amount of knowledge and resources, so if there's something you're curious about or need help with, feel free to ask! Some examples of things I can help with include:\n",
      "\n",
      "* Answering questions on a variety of topics such as history, science, technology, health, and more\n",
      "* Providing guidance on how to complete a task or project\n",
      "* Offering suggestions and ideas for creative projects or brainstorming sessions\n",
      "* Assisting with language translation and communication\n",
      "* And much more!\n",
      "\n",
      "Is there anything specific you would like to know or discuss? Please don't hesitate to ask, and I'll do my best to assist you.\n"
     ]
    }
   ],
   "source": [
    "# chat history not passed so Llama doesn't have the context and doesn't know this is more about the book\n",
    "followup = \"tell me more\"\n",
    "followup_answer = llm(followup)\n",
    "print(followup_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aeaffc7",
   "metadata": {},
   "source": [
    "[`ConversationBufferMemory`](https://python.langchain.com/docs/modules/memory/types/buffer) is used to pass the chat history to the model and give it the capability to handle follow up questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3b5f8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5428ca27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Finance Act 2024 of Kenya is a legislation that outlines various tax measures, amendments, and reforms aimed at promoting economic growth, increasing revenue, and enhancing transparency and accountability in the country's financial system."
     ]
    }
   ],
   "source": [
    "# To stream the output\n",
    "import replicate \n",
    "\n",
    "for event in replicate.stream(llm, input=input):\n",
    "    print(event, end=\"\")\n",
    "    # Update the chat history\n",
    "    chat_history.append({\"user\": input[\"prompt\"], \"assistant\": event})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9784127",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "2 validation errors for ConversationChain\nllm\n  instance of Runnable expected (type=type_error.arbitrary_type; expected_arbitrary_type=Runnable)\nllm\n  instance of Runnable expected (type=type_error.arbitrary_type; expected_arbitrary_type=Runnable)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmemory\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConversationBufferMemory\n\u001b[1;32m      5\u001b[0m memory \u001b[38;5;241m=\u001b[39m ConversationBufferMemory()\n\u001b[0;32m----> 6\u001b[0m conversation \u001b[38;5;241m=\u001b[39m \u001b[43mConversationChain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/cloud_llama/lib/python3.9/site-packages/langchain_core/_api/deprecation.py:183\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    182\u001b[0m     emit_warning()\n\u001b[0;32m--> 183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/cloud_llama/lib/python3.9/site-packages/pydantic/v1/main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[0;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
      "\u001b[0;31mValidationError\u001b[0m: 2 validation errors for ConversationChain\nllm\n  instance of Runnable expected (type=type_error.arbitrary_type; expected_arbitrary_type=Runnable)\nllm\n  instance of Runnable expected (type=type_error.arbitrary_type; expected_arbitrary_type=Runnable)"
     ]
    }
   ],
   "source": [
    "# using ConversationBufferMemory to pass memory (chat history) for follow up questions\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory = memory,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e9af5f",
   "metadata": {},
   "source": [
    "Once this is set up, let us repeat the steps from before and ask the model a simple question.\n",
    "\n",
    "Then we pass the question and answer back into the model for context along with the follow up question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "baee2d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Ah, you're asking about \"The Innovator's Dilemma,\" the iconic book written by Clayton Christensen! He is a renowned author, professor, and business consultant known for his groundbreaking work in the fields of innovation and disruptive technologies.\n",
      "\n",
      "Published in 1997, \"The Innovator's Dilemma\" explores why successful companies often struggle to adapt to new technologies and business models that ultimately disrupt their industries. The book introduces the concept of \"disruptive innovation,\" which describes how small, unassuming ideas can eventually upend entire markets and leave established players behind.\n",
      "\n",
      "Clayton Christensen has since become one of the most influential voices in the business world, and his ideas have been applied across various sectors, from technology and healthcare to finance and education. His follow-up books, such as \"The Innovator's Solution\" and \"Competing Against Luck,\" further expand on these concepts and offer practical guidance for leaders looking to stay ahead of the curve.\n",
      "\n",
      "So there you have it – the brilliant mind behind \"The Innovator's Dilemma\" is none other than Clayton Christensen!\n"
     ]
    }
   ],
   "source": [
    "# restart from the original question\n",
    "answer = conversation.predict(input=question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c7d67a8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'memory' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# pass context (previous question and answer) along with the follow up \"tell me more\" to Llama who now knows more of what\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmemory\u001b[49m\u001b[38;5;241m.\u001b[39msave_context({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: question},\n\u001b[1;32m      3\u001b[0m                     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m: answer})\n\u001b[1;32m      4\u001b[0m followup_answer \u001b[38;5;241m=\u001b[39m conversation\u001b[38;5;241m.\u001b[39mpredict(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mfollowup)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(followup_answer)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'memory' is not defined"
     ]
    }
   ],
   "source": [
    "# pass context (previous question and answer) along with the follow up \"tell me more\" to Llama who now knows more of what\n",
    "memory.save_context({\"input\": question},\n",
    "                    {\"output\": answer})\n",
    "followup_answer = conversation.predict(input=followup)\n",
    "print(followup_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99516a1a",
   "metadata": {},
   "source": [
    "Load the documents from the data path\n",
    "\n",
    "Using Llama 2 to answer questions using documents for context. \n",
    "This gives us the ability to update Llama 2's knowledge thus giving it better context without needing to fine-tune. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c61ceb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "\n",
    "DATA_PATH = 'data'\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Remove headers, footers, and other irrelevant content\n",
    "    # Customize this function based on your specific requirements\n",
    "    processed_text = text.strip()\n",
    "    # Add more preprocessing steps as needed\n",
    "    return processed_text\n",
    "\n",
    "# Load and preprocess the documents\n",
    "loader = PyPDFDirectoryLoader(DATA_PATH)\n",
    "docs = loader.load_and_split()\n",
    "\n",
    "# Preprocess each document\n",
    "preprocessed_docs = [preprocess_text(doc.page_content) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2cf3f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of PDF files: 1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Check the number of PDF files in the data directory\n",
    "pdf_files = [f for f in os.listdir(DATA_PATH) if f.endswith('.pdf')]\n",
    "print(f\"Number of PDF files: {len(pdf_files)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b8268e",
   "metadata": {},
   "source": [
    "Storing the documents. There are more than 30 vector stores (DBs) supported by LangChain. \n",
    "In this case [Chroma](https://python.langchain.com/docs/integrations/vectorstores/chroma) is used which is light-weight and in memory so it's easy to get started with.\n",
    "\n",
    "We will also import the HuggingFaceEmbeddings and RecursiveCharacterTextSplitter to assist in storing the documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8666d03",
   "metadata": {},
   "source": [
    "Version 1: ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eecb6a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m chunks \u001b[38;5;241m=\u001b[39m text_splitter\u001b[38;5;241m.\u001b[39msplit_documents(docs)\n\u001b[1;32m      8\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m HuggingFaceEmbeddings()\n\u001b[0;32m----> 9\u001b[0m db \u001b[38;5;241m=\u001b[39m \u001b[43mChroma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m retriever \u001b[38;5;241m=\u001b[39m db\u001b[38;5;241m.\u001b[39mas_retriever()\n",
      "File \u001b[0;32m~/miniforge3/envs/cloud_llama/lib/python3.9/site-packages/langchain_community/vectorstores/chroma.py:778\u001b[0m, in \u001b[0;36mChroma.from_documents\u001b[0;34m(cls, documents, embedding, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[1;32m    776\u001b[0m texts \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[1;32m    777\u001b[0m metadatas \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m--> 778\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    779\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    780\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    783\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    784\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpersist_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient_settings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/cloud_llama/lib/python3.9/site-packages/langchain_community/vectorstores/chroma.py:736\u001b[0m, in \u001b[0;36mChroma.from_texts\u001b[0;34m(cls, texts, embedding, metadatas, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[1;32m    728\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mchromadb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbatch_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_batches\n\u001b[1;32m    730\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m create_batches(\n\u001b[1;32m    731\u001b[0m         api\u001b[38;5;241m=\u001b[39mchroma_collection\u001b[38;5;241m.\u001b[39m_client,\n\u001b[1;32m    732\u001b[0m         ids\u001b[38;5;241m=\u001b[39mids,\n\u001b[1;32m    733\u001b[0m         metadatas\u001b[38;5;241m=\u001b[39mmetadatas,\n\u001b[1;32m    734\u001b[0m         documents\u001b[38;5;241m=\u001b[39mtexts,\n\u001b[1;32m    735\u001b[0m     ):\n\u001b[0;32m--> 736\u001b[0m         \u001b[43mchroma_collection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_texts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    737\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtexts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    738\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    739\u001b[0m \u001b[43m            \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    740\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    742\u001b[0m     chroma_collection\u001b[38;5;241m.\u001b[39madd_texts(texts\u001b[38;5;241m=\u001b[39mtexts, metadatas\u001b[38;5;241m=\u001b[39mmetadatas, ids\u001b[38;5;241m=\u001b[39mids)\n",
      "File \u001b[0;32m~/miniforge3/envs/cloud_llama/lib/python3.9/site-packages/langchain_community/vectorstores/chroma.py:275\u001b[0m, in \u001b[0;36mChroma.add_texts\u001b[0;34m(self, texts, metadatas, ids, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(texts)\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embedding_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 275\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embedding_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m metadatas:\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;66;03m# fill metadatas with empty dicts if somebody\u001b[39;00m\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;66;03m# did not specify metadata for all texts\u001b[39;00m\n\u001b[1;32m    279\u001b[0m     length_diff \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(texts) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(metadatas)\n",
      "File \u001b[0;32m~/miniforge3/envs/cloud_llama/lib/python3.9/site-packages/langchain_community/embeddings/huggingface.py:93\u001b[0m, in \u001b[0;36mHuggingFaceEmbeddings.embed_documents\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m     91\u001b[0m     sentence_transformers\u001b[38;5;241m.\u001b[39mSentenceTransformer\u001b[38;5;241m.\u001b[39mstop_multi_process_pool(pool)\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 93\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow_progress\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_kwargs\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[0;32m~/miniforge3/envs/cloud_llama/lib/python3.9/site-packages/sentence_transformers/SentenceTransformer.py:373\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[0;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[1;32m    371\u001b[0m             \u001b[38;5;66;03m# fixes for #522 and #487 to avoid oom problems on gpu with large datasets\u001b[39;00m\n\u001b[1;32m    372\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m convert_to_numpy:\n\u001b[0;32m--> 373\u001b[0m                 embeddings \u001b[38;5;241m=\u001b[39m \u001b[43membeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    375\u001b[0m         all_embeddings\u001b[38;5;241m.\u001b[39mextend(embeddings)\n\u001b[1;32m    377\u001b[0m all_embeddings \u001b[38;5;241m=\u001b[39m [all_embeddings[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m np\u001b[38;5;241m.\u001b[39margsort(length_sorted_idx)]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=100)\n",
    "chunks = text_splitter.split_documents(docs)\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "db = Chroma.from_documents(chunks, embeddings)\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e5404e",
   "metadata": {},
   "source": [
    "Version 2: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7bfe7d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# Split the documents into chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=4000, chunk_overlap=200)\n",
    "chunks = text_splitter.create_documents(preprocessed_docs)\n",
    "\n",
    "# Create the vector database\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "db = FAISS.from_documents(chunks, embeddings)\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d4a17c",
   "metadata": {},
   "source": [
    "To store the documents, we will need to split them into chunks using [`RecursiveCharacterTextSplitter`](https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/recursive_text_splitter) and create vector representations of these chunks using [`HuggingFaceEmbeddings`](https://www.google.com/search?q=langchain+hugging+face+embeddings&sca_esv=572890011&ei=ARUoZaH4LuumptQP48ah2Ac&oq=langchian+hugg&gs_lp=Egxnd3Mtd2l6LXNlcnAiDmxhbmdjaGlhbiBodWdnKgIIADIHEAAYgAQYCjIHEAAYgAQYCjIHEAAYgAQYCjIHEAAYgAQYCjIHEAAYgAQYCjIHEAAYgAQYCjIHEAAYgAQYCjIHEAAYgAQYCjIHEAAYgAQYCjIHEAAYgAQYCkjeHlC5Cli5D3ABeAGQAQCYAV6gAb4CqgEBNLgBAcgBAPgBAcICChAAGEcY1gQYsAPiAwQYACBBiAYBkAYI&sclient=gws-wiz-serp) on them before storing them into our vector database. \n",
    "\n",
    "In general, you should use larger chuck sizes for highly structured text such as code and smaller size for less structured text. You may need to experiment with different chunk sizes and overlap values to find out the best numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc65e161",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=100)\n",
    "chunks = text_splitter.split_documents(docs)\n",
    "\n",
    "# create the vector db to store all the split chunks as embeddings\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "db = Chroma.from_documents(chunks, embeddings)\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ad02d7",
   "metadata": {},
   "source": [
    "We then use ` RetrievalQA` to retrieve the documents from the vector database and give the model more context on Llama 2, thereby increasing its knowledge.\n",
    "\n",
    "For each question, LangChain performs a semantic similarity search of it in the vector db, then passes the search results as the context to Llama to answer the question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b33d638",
   "metadata": {},
   "source": [
    "Retriever 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ab5ecd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms.base import LLM\n",
    "\n",
    "# Create a prompt template\n",
    "template = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "You are a helpful assistant that answers questions based law and legal documents.\n",
    "If the context does not provide enough information to answer the question, answer to the best of your ability using your pretrained data. Remember you are a Law based Chatbot\"\n",
    "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "class CustomLLMReplicate(LLM):\n",
    "    def _call(self, prompt, stop=None):\n",
    "        return llm_replicate(prompt)\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self):\n",
    "        return {\"name\": \"custom_llm_replicate\"}\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self):\n",
    "        return \"custom\"\n",
    "\n",
    "def llm_replicate(prompt, **kwargs):\n",
    "    input_data = {\n",
    "        \"prompt\": prompt,\n",
    "        \"temperature\": 0.75,\n",
    "        \"top_p\": 0.9,\n",
    "        \"max_tokens\": 800,\n",
    "    }\n",
    "    output = replicate.run(llm, input=input_data)\n",
    "    return \"\".join(output)\n",
    "\n",
    "# Create an instance of CustomLLMReplicate\n",
    "custom_llm_replicate = CustomLLMReplicate()\n",
    "\n",
    "# Create a question-answering chain\n",
    "qa_chain = load_qa_chain(llm=custom_llm_replicate, chain_type=\"stuff\", prompt=prompt)\n",
    "\n",
    "def ask_question(query):\n",
    "    preprocessed_docs = retriever.get_relevant_documents(query)\n",
    "    \n",
    "    if not preprocessed_docs:\n",
    "        # If no relevant documents are found, use the Replicate API with Llama3\n",
    "        return llm_replicate(query)\n",
    "    \n",
    "    result = qa_chain.run(input_documents=preprocessed_docs, question=query)\n",
    "    return result\n",
    "\n",
    "# Example usage\n",
    "query = \"What are the amendments made in the Climate Change (Amendment) Act Kenya 2023\"\n",
    "answer = ask_question(query)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2beb4f8a",
   "metadata": {},
   "source": [
    "Retriever 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "af2be910",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'list' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 88\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m     87\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat are the amendments made in the Climate Change (Amendment) Act Kenya 2023, answer as briefly as possible\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 88\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[43mask_question\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28mprint\u001b[39m(answer)\n",
      "Cell \u001b[0;32mIn[32], line 84\u001b[0m, in \u001b[0;36mask_question\u001b[0;34m(query)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     82\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m result \u001b[38;5;241m=\u001b[39m qa_chain\u001b[38;5;241m.\u001b[39mrun(input_documents\u001b[38;5;241m=\u001b[39m[\u001b[43mpreprocessed_docs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpage_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m], question\u001b[38;5;241m=\u001b[39mquery)\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mTypeError\u001b[0m: 'list' object is not callable"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms.base import LLM\n",
    "\n",
    "DATA_PATH = 'data'\n",
    "\n",
    "def preprocess_text(text):\n",
    "    processed_text = text.strip()\n",
    "    return processed_text\n",
    "\n",
    "loader = PyPDFDirectoryLoader(DATA_PATH)\n",
    "docs = loader.load_and_split()\n",
    "preprocessed_docs = [preprocess_text(doc.page_content) for doc in docs]\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=4000, chunk_overlap=200)\n",
    "chunks = text_splitter.create_documents(preprocessed_docs)\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "db = FAISS.from_documents(chunks, embeddings)\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "template = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "You are a helpful assistant that answers questions based on law and legal documents.\n",
    "If the context does not provide enough information to answer the question, answer to the best of your ability using your pretrained data. Remember you are a Law based Chatbot.\n",
    "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "class CustomLLMReplicate(LLM):\n",
    "    def _call(self, prompt, stop=None):\n",
    "        return llm_replicate(prompt)\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self):\n",
    "        return {\"name\": \"custom_llm_replicate\"}\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self):\n",
    "        return \"custom\"\n",
    "\n",
    "def llm_replicate(prompt, **kwargs):\n",
    "    input_data = {\n",
    "        \"prompt\": prompt,\n",
    "        \"temperature\": 0.75,\n",
    "        \"top_p\": 0.9,\n",
    "        \"max_tokens\": 1200,\n",
    "    }\n",
    "    output = replicate.run(llm, input=input_data)\n",
    "    return \"\".join(output)\n",
    "\n",
    "custom_llm_replicate = CustomLLMReplicate()\n",
    "qa_chain = load_qa_chain(llm=custom_llm_replicate, chain_type=\"stuff\", prompt=prompt)\n",
    "\n",
    "def ask_question(query):\n",
    "    preprocessed_docs = retriever.get_relevant_documents(query)\n",
    "    \n",
    "    if not preprocessed_docs:\n",
    "        llm_prompt = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "You are a helpful assistant that answers questions based on law and legal documents.\n",
    "If the context does not provide enough information to answer the question, answer to the best of your ability using your pretrained data. Remember you are a Law based Chatbot.\n",
    "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "Question: {query}\n",
    "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "        return llm_replicate(llm_prompt)\n",
    "    \n",
    "    result = qa_chain.run(input_documents=preprocessed_docs, question=query)\n",
    "    return result\n",
    "\n",
    "query = \"What are the amendments made in the Climate Change (Amendment) Act Kenya 2023, answer as briefly as possible\"\n",
    "answer = ask_question(query)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ade0e5d6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rag_chain' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat are the amendments made in the Climate Change (Amendment) Act Kenya 2023\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m chat_history \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m replicate\u001b[38;5;241m.\u001b[39mstream(llm, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[43mrag_chain\u001b[49m\u001b[38;5;241m.\u001b[39mrun(query)):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(output, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m     chat_history\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: query,  \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m: output})\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rag_chain' is not defined"
     ]
    }
   ],
   "source": [
    "query = \"What are the amendments made in the Climate Change (Amendment) Act Kenya 2023\"\n",
    "chat_history = []\n",
    "\n",
    "for output in replicate.stream(llm, input=rag_chain.run(query)):\n",
    "    print(output, end=\"\", flush=True)\n",
    "    chat_history.append({\"user\": query,  \"assistant\": output})\n",
    "\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e7cdff82",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'dict' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m----> 2\u001b[0m     follow_up_query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAsk a follow-up question (or type \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mexit\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m to quit): \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m follow_up_query\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexit\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'dict' object is not callable"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    follow_up_query = input(\"Ask a follow-up question (or type 'exit' to quit): \")\n",
    "    if follow_up_query.lower() == \"exit\":\n",
    "        break\n",
    "    \n",
    "    template = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "You are a helpful assistant that answers questions based on the given context and the conversation history.\n",
    "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Conversation History:\n",
    "{history}\n",
    "\n",
    "Follow-up Question: {question}\n",
    "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "    prompt = PromptTemplate(template=template, input_variables=[\"context\", \"history\", \"question\"])\n",
    "    \n",
    "    docs = retriever.get_relevant_documents(follow_up_query)\n",
    "    context = \" \".join([doc.page_content for doc in docs])\n",
    "    \n",
    "    history = \"\\n\".join([f\"User: {msg['user']}\\nAssistant: {msg['assistant']}\" for msg in chat_history])\n",
    "    \n",
    "    llm_input = prompt.format(context=context, history=history, question=follow_up_query)\n",
    "    \n",
    "    input_data = {\n",
    "        \"prompt\": llm_input,\n",
    "        \"temperature\": 0.75,\n",
    "        \"top_p\": 0.9,\n",
    "        \"max_tokens\": 800,\n",
    "    }\n",
    "    \n",
    "    result = \"\"\n",
    "    for output in replicate.stream(llm, input=input_data):\n",
    "        print(output, end=\"\", flush=True)\n",
    "        result += output\n",
    "    \n",
    "    chat_history.append({\"user\": follow_up_query, \"assistant\": result})\n",
    "    \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00e3f72b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Based on the information provided, I can see that the Climate Change (Amendment) Act Kenya 2023 has undergone several amendments. However, without access to the full text of the act, I cannot provide a comprehensive list of all the amendments made.\n",
      "\n",
      "Some of the amendments mentioned in the context include:\n",
      "\n",
      "* Insertion of a new definition for \"Central Authority\" in the Anti-Money Laundering and Combating of Terrorism Financing Laws (Amendment) Act, 2023.\n",
      "* Amendment of section 7 of the Kenya Roads Board Act, 1999 to delete paragraph (g) and substitute it with a new paragraph (ga).\n",
      "* Amendment of section 35 of the Kenya Roads Board Act, 1999 to insert a new subsection (2A) requiring the annual estimates to be submitted together with a collated annual roads programme.\n",
      "* Amendment of the First Schedule to the Kenya Roads Board Act, 1999 to delete paragraphs 4, 5, and 6.\n",
      "* Amendment of section 5 of the Kenya Revenue Authority Act, 1995 to delete the words \"for the better carrying out of its functions\" and substitute them with the words \"the staff of the Authority, general public and other jurisdictions\".\n",
      "* Amendment of section 34 of the Value Added Tax Act, 2013 to delete the proviso and substitute it with a new proviso regarding the registration of persons supplying imported digital services over the internet.\n",
      "* Amendment of section 43 of the Value Added Tax Act, 2013 to delete the words \"in Kenya\".\n",
      "* Amendment of the First Schedule to the Value Added Tax Act, 2013 to make changes to the list of goods and services subject to value added tax.\n",
      "\n",
      "Again, without access to the full text of the act, I cannot provide a comprehensive list of all the amendments made.\n"
     ]
    }
   ],
   "source": [
    "# use LangChain's RetrievalQA, to associate Llama with the loaded documents stored in the vector db\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectordb.as_retriever()\n",
    ")\n",
    "\n",
    "\n",
    "question = \"What are the amendments made in the Climate Change (Amendment) Act Kenya 2023\"\n",
    "result = qa_chain({\"query\": question})\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e63769a",
   "metadata": {},
   "source": [
    "Now, lets bring it all together by incorporating follow up questions.\n",
    "\n",
    "First we ask a follow up questions without giving the model context of the previous conversation. \n",
    "Without this context, the answer we get does not relate to our original question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53f27473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Based on the provided text, the following amendments are relevant to climate change:\n",
      "\n",
      "1. Amendment of section 28 of the Unclaimed Financial Assets Act, 2011, which inserts the words \"or such other person as the claimant may designate\" immediately after the word \"claimant\". This amendment allows for the designation of a third party within a group which has existed for at least twenty-four months.\n",
      "2. Amendment of the Ninth Schedule to the Income Tax Act, which deletes the words \"ten per cent\" appearing in subparagraph (1) and substitutes them with \"twenty per cent\". This increase in the tax rate for certain activities may have an impact on climate change efforts.\n",
      "3. Amendment of section 5 of the Value Added Tax Act, 2013, which deletes paragraph (aa) and (ab). These changes may affect the taxation of certain goods and services related to climate change mitigation and adaptation efforts.\n"
     ]
    }
   ],
   "source": [
    "# no context passed so Llama2 doesn't have enough context to answer so it lets its imagination go wild\n",
    "result = qa_chain({\"query\": \"Provide only the amendments that are relevant to climate change\"})\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833221c0",
   "metadata": {},
   "source": [
    "As we did before, let us use the `ConversationalRetrievalChain` package to give the model context of our previous question so we can add follow up questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "743644a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use ConversationalRetrievalChain to pass chat history for follow up questions\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "chat_chain = ConversationalRetrievalChain.from_llm(llm, vectordb.as_retriever(), return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7c3d1142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The text you provided does not contain the word \"ll\n"
     ]
    }
   ],
   "source": [
    "# let's ask the original question \"What is llama2?\" again\n",
    "result = chat_chain({\"question\": question, \"chat_history\": []})\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4b17f08f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I don't know the answer to that question. Llama2 is not a term I am familiar with, and I cannot provide information on its typical use cases.\n"
     ]
    }
   ],
   "source": [
    "# this time we pass chat history along with the follow up so good things should happen\n",
    "chat_history = [(question, result[\"answer\"])]\n",
    "followup = \"what are its use cases?\"\n",
    "followup_answer = chat_chain({\"question\": followup, \"chat_history\": chat_history})\n",
    "print(followup_answer['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f4eabf",
   "metadata": {},
   "source": [
    "Further follow ups can be made possible by updating chat_history.\n",
    "\n",
    "Note that results can get cut off. You may set \"max_new_tokens\" in the Replicate call above to a larger number (like shown below) to avoid the cut off.\n",
    "\n",
    "```python\n",
    "model_kwargs={\"temperature\": 0.01, \"top_p\": 1, \"max_new_tokens\": 1000}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d22347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# further follow ups can be made possible by updating chat_history like this:\n",
    "chat_history.append((followup, followup_answer[\"answer\"]))\n",
    "more_followup = \"what tasks can it assist with?\"\n",
    "more_followup_answer = chat_chain({\"question\": more_followup, \"chat_history\": chat_history})\n",
    "print(more_followup_answer['answer'])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
